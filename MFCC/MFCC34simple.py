# MFCC4,5ã®ã¿ã‚’ä½¿ç”¨ã—ãŸæ¯éŸ³èªè­˜ã‚·ã‚¹ãƒ†ãƒ 
# === ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ ===
import os
import numpy as np
import sounddevice as sd
import soundfile as sf
import librosa
import matplotlib.pyplot as plt
from scipy.signal import find_peaks
from time import sleep
import queue

# === ãƒ•ã‚©ãƒ³ãƒˆè¨­å®šï¼ˆMacå¯¾å¿œãƒ»IPAè¨˜å·å¯¾å¿œï¼‰ ===
# IPAè¨˜å·ãŒè¡¨ç¤ºã§ãã‚‹ãƒ•ã‚©ãƒ³ãƒˆã‚’è¨­å®š
plt.rcParams['font.family'] = ['DejaVu Sans', 'Arial Unicode MS', 'Lucida Grande', 'AppleGothic']

# === éŸ³å£°å‡¦ç†è¨­å®š ===
RATE = 16000  # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆï¼ˆ16kHzï¼‰
DURATION = 1.0  # éŒ²éŸ³æ™‚é–“ï¼ˆç§’ï¼‰
N_MFCC = 13  # MFCCã®æ¬¡å…ƒæ•°
RECORDINGS_DIR = "recordings_formant"  # éŒ²éŸ³ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜å…ˆ
JAPANESE_VOWELS = ['a', 'i', 'u', 'e', 'o']  # æ—¥æœ¬èªæ¯éŸ³
ENGLISH_VOWELS = ['Ã¦', 'Éª', 'ÊŠ', 'É›', 'É”', 'ÊŒ', 'É‘']  # è‹±èªæ¯éŸ³ï¼ˆcat, sit, put, get, caught, but, fatherï¼‰
VOWELS = JAPANESE_VOWELS + ENGLISH_VOWELS  # å…¨ã¦ã®å¯¾è±¡æ¯éŸ³
SAMPLES_PER_VOWEL = 3  # æ¯éŸ³ã”ã¨ã®ã‚µãƒ³ãƒ—ãƒ«æ•°

# é¡ä¼¼åº¦åˆ¤å®šã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
VERY_SIMILAR_THRESHOLD = 0.85  # ã€Œã™ã”ãè¿‘ã„ã€ã¨åˆ¤æ–­ã™ã‚‹é¡ä¼¼åº¦é–¾å€¤

# === æ¯éŸ³ã®è‰²ãƒãƒƒãƒ”ãƒ³ã‚° ===
COLOR_MAP = {
    # æ—¥æœ¬èªæ¯éŸ³
    'a': 'red',
    'i': 'blue', 
    'u': 'green',
    'e': 'purple',
    'o': 'orange',
    # è‹±èªæ¯éŸ³
    'Ã¦': 'darkred',     # cat
    'Éª': 'darkblue',    # sit
    'ÊŠ': 'darkgreen',   # put
    'É›': 'darkorchid',  # get
    'É”': 'darkorange',  # caught
    'ÊŒ': 'brown',       # but
    'É‘': 'maroon',      # father
    'É™': 'gray'         # æ›–æ˜§æ¯éŸ³ï¼ˆã‚·ãƒ¥ãƒ¯ãƒ¼ï¼‰
}



# === æ¯éŸ³ã”ã¨ã®ç™ºéŸ³ã‚¢ãƒ‰ãƒã‚¤ã‚¹ ===
ADVICE_MAP = {
    # æ—¥æœ¬èªæ¯éŸ³
    'a': "å£ã‚’å¤§ããç¸¦ã«é–‹ã‘ã€èˆŒã¯ä¸‹ã«è½ã¨ã—ã¾ã—ã‚‡ã†ã€‚",
    'i': "å£ã‚’æ¨ªã«å¼•ã„ã¦ã€èˆŒã¯å‰ã«å‡ºã™ã‚ˆã†ã«ã—ã¾ã—ã‚‡ã†ã€‚",
    'u': "å”‡ã‚’ã™ã¼ã‚ã¦ã€èˆŒã‚’å¾Œã‚ã«å¼•ãã¾ã—ã‚‡ã†ã€‚",
    'e': "å£è§’ã‚’å°‘ã—ä¸Šã’ã€èˆŒã‚’ã‚„ã‚„å‰ã«å‡ºã—ã¾ã—ã‚‡ã†ã€‚",
    'o': "å”‡ã‚’ä¸¸ãçªãå‡ºã—ã€èˆŒã‚’å¾Œã‚ã«å¼•ãã¾ã—ã‚‡ã†ã€‚",
    # è‹±èªæ¯éŸ³
    'Ã¦': "å£ã‚’æ¨ªã«å¤§ããé–‹ãã€èˆŒã‚’ä½ãå‰ã«ã€‚'cat'ã®éŸ³ã€‚",
    'Éª': "å£ã‚’ã‚„ã‚„é–‹ãã€èˆŒã‚’ä¸­å¤®ã«ã€‚'sit'ã®çŸ­ã„éŸ³ã€‚",
    'ÊŠ': "å”‡ã‚’è»½ãä¸¸ã‚ã€èˆŒã‚’ä¸­å¤®å¾Œæ–¹ã«ã€‚'put'ã®éŸ³ã€‚",
    'É›': "å£ã‚’ä¸­ç¨‹åº¦ã«é–‹ãã€èˆŒã‚’ä¸­å¤®å‰æ–¹ã«ã€‚'get'ã®éŸ³ã€‚",
    'É”': "å”‡ã‚’ä¸¸ãé–‹ãã€èˆŒã‚’ä½ãå¾Œæ–¹ã«ã€‚'caught'ã®éŸ³ã€‚",
    'ÊŒ': "å£ã‚’ãƒªãƒ©ãƒƒã‚¯ã‚¹ã—ã¦é–‹ãã€èˆŒã‚’ä¸­å¤®ã«ã€‚'but'ã®éŸ³ã€‚",
    'É‘': "å£ã‚’å¤§ããé–‹ãã€èˆŒã‚’ä½ãå¾Œæ–¹ã«ã€‚'father'ã®éŸ³ã€‚",
    'É™': "å£ã¨èˆŒã‚’ãƒªãƒ©ãƒƒã‚¯ã‚¹ã•ã›ã€åŠ›ã‚’æŠœã„ã¦ç™ºéŸ³ã—ã¾ã—ã‚‡ã†ã€‚"
}

# === ãƒ•ã‚©ãƒ«ãƒãƒ³ãƒˆæŠ½å‡ºæ©Ÿèƒ½ ===
def extract_formants(y, sr, n_formants=3):
    """éŸ³å£°æ³¢å½¢ã‹ã‚‰ãƒ•ã‚©ãƒ«ãƒãƒ³ãƒˆå‘¨æ³¢æ•°ã‚’æŠ½å‡ºã™ã‚‹ï¼ˆLPCåˆ†æä½¿ç”¨ï¼‰"""
    from scipy.signal import freqz
    from scipy.signal.windows import hamming
    
    # éŸ³å£°ä¿¡å·ã®å‰å‡¦ç†
    # ãƒ—ãƒªã‚¨ãƒ³ãƒ•ã‚¡ã‚·ã‚¹ï¼ˆé«˜å‘¨æ³¢æˆåˆ†ã®å¼·èª¿ï¼‰
    pre_emphasis = 0.97
    emphasized = np.append(y[0], y[1:] - pre_emphasis * y[:-1])
    
    # ãƒ•ãƒ¬ãƒ¼ãƒ åˆ†å‰²ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    frame_size = int(0.025 * sr)  # 25msã®ãƒ•ãƒ¬ãƒ¼ãƒ 
    frame_shift = int(0.010 * sr)  # 10msã®ã‚·ãƒ•ãƒˆ
    
    formants_list = []
    
    # ãƒ•ãƒ¬ãƒ¼ãƒ ã”ã¨ã«å‡¦ç†
    for start in range(0, len(emphasized) - frame_size, frame_shift):
        frame = emphasized[start:start + frame_size]
        
        # ãƒãƒŸãƒ³ã‚°çª“ã‚’é©ç”¨
        windowed = frame * hamming(len(frame))
        
        # LPCåˆ†æ
        # LPCæ¬¡æ•°ã¯ (ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆ/1000) + 2 ãŒç›®å®‰
        lpc_order = int(sr / 1000) + 4
        
        try:
            # è‡ªå·±ç›¸é–¢æ³•ã«ã‚ˆã‚‹LPCä¿‚æ•°ã®è¨ˆç®—
            autocorr = np.correlate(windowed, windowed, mode='full')
            autocorr = autocorr[len(autocorr)//2:]
            
            # Levinson-Durbinå†å¸°ã§LPCä¿‚æ•°ã‚’è¨ˆç®—
            lpc_coeffs = solve_lpc(autocorr, lpc_order)
            
            # LPCã‚¹ãƒšã‚¯ãƒˆãƒ«åŒ…çµ¡ã‚’è¨ˆç®—
            w, h = freqz([1], lpc_coeffs, worN=8192, fs=sr)
            
            # ã‚¹ãƒšã‚¯ãƒˆãƒ«åŒ…çµ¡ã‹ã‚‰ãƒ”ãƒ¼ã‚¯ã‚’æ¤œå‡º
            magnitude = 20 * np.log10(np.abs(h) + 1e-15)
            
            # ãƒ”ãƒ¼ã‚¯æ¤œå‡ºï¼ˆãƒ•ã‚©ãƒ«ãƒãƒ³ãƒˆå€™è£œï¼‰
            peaks, _ = find_peaks(magnitude, distance=int(300/(sr/len(w))))
            
            # ãƒ”ãƒ¼ã‚¯ã®å‘¨æ³¢æ•°ã‚’å–å¾—
            peak_freqs = w[peaks]
            
            # æœ‰åŠ¹ãªãƒ•ã‚©ãƒ«ãƒãƒ³ãƒˆç¯„å›²ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆ200Hz-5000Hzï¼‰
            valid_peaks = [(f, magnitude[peaks[i]]) for i, f in enumerate(peak_freqs) 
                          if 200 < f < 5000]
            
            # å¼·åº¦ã§ã‚½ãƒ¼ãƒˆã—ã¦ä¸Šä½nå€‹ã‚’é¸æŠ
            valid_peaks.sort(key=lambda x: x[1], reverse=True)
            
            # å‘¨æ³¢æ•°ã§ã‚½ãƒ¼ãƒˆï¼ˆä½ã„é †ï¼‰ã—ã¦ãƒ•ã‚©ãƒ«ãƒãƒ³ãƒˆã¨ã™ã‚‹
            if valid_peaks:
                formant_freqs = sorted([f for f, _ in valid_peaks[:n_formants*2]])[:n_formants]
            else:
                formant_freqs = []
            
            # å¿…è¦ãªæ•°ã«æº€ãŸãªã„å ´åˆã¯0ã§åŸ‹ã‚ã‚‹
            formant_freqs = formant_freqs + [0] * (n_formants - len(formant_freqs))
            
        except Exception:
            # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
            formant_freqs = [0] * n_formants
        
        formants_list.append(formant_freqs)
    
    # å…¨ãƒ•ãƒ¬ãƒ¼ãƒ ã®ãƒ•ã‚©ãƒ«ãƒãƒ³ãƒˆã®ä¸­å¤®å€¤ã‚’è¨ˆç®—
    formants_array = np.array(formants_list)
    # 0ä»¥å¤–ã®å€¤ã®ã¿ã§ä¸­å¤®å€¤ã‚’è¨ˆç®—
    median_formants = []
    for i in range(n_formants):
        valid_values = formants_array[:, i][formants_array[:, i] > 0]
        if len(valid_values) > 0:
            median_formants.append(np.median(valid_values))
        else:
            median_formants.append(0)
    
    return np.array(median_formants)

def solve_lpc(autocorr, order):
    """Levinson-Durbinå†å¸°ã«ã‚ˆã‚‹LPCä¿‚æ•°ã®è¨ˆç®—"""
    # åˆæœŸåŒ–
    error = autocorr[0]
    lpc = np.zeros(order + 1)
    lpc[0] = 1.0
    
    for i in range(1, order + 1):
        # åå°„ä¿‚æ•°ã®è¨ˆç®—
        k = -np.sum(lpc[:i] * autocorr[i:0:-1]) / error
        
        # LPCä¿‚æ•°ã®æ›´æ–°
        lpc_temp = lpc.copy()
        lpc[i] = k
        for j in range(1, i):
            lpc[j] = lpc_temp[j] + k * lpc_temp[i - j]
        
        # äºˆæ¸¬èª¤å·®ã®æ›´æ–°
        error *= (1 - k * k)
        
        if error <= 0:
            break
    
    return lpc


# === éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰MFCCç‰¹å¾´é‡ã¨ãƒ•ã‚©ãƒ«ãƒãƒ³ãƒˆã‚’æŠ½å‡º ===
def extract_features():
    X_mfcc, X_formants, y = [], [], []
    all_formants = {}  # æ¯éŸ³ã”ã¨ã®å€‹åˆ¥ã‚µãƒ³ãƒ—ãƒ«ã®ãƒ•ã‚©ãƒ«ãƒãƒ³ãƒˆã‚’ä¿å­˜
    
    # æ—¥æœ¬èªæ¯éŸ³ã®å‡¦ç†
    for vowel in JAPANESE_VOWELS:
        all_formants[vowel] = []  # ã“ã®æ¯éŸ³ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ•ã‚©ãƒ«ãƒãƒ³ãƒˆãƒªã‚¹ãƒˆ
        
        file_prefix = vowel
        
        samples_found = 0
        for i in range(1, SAMPLES_PER_VOWEL + 1):
            filepath = os.path.join(RECORDINGS_DIR, f"{file_prefix}_{i}.wav")
            if os.path.exists(filepath):
                # éŸ³å£°èª­ã¿è¾¼ã¿
                y_data, sr = librosa.load(filepath, sr=RATE)
                
                # ç„¡éŸ³åŒºé–“é™¤å»
                y_data, _ = librosa.effects.trim(y_data, top_db=20)
                
                # MFCCæŠ½å‡º
                mfcc = librosa.feature.mfcc(y=y_data, sr=sr, n_mfcc=N_MFCC)
                mfcc_mean = np.mean(mfcc, axis=1)
                # MFCC4ã¨MFCC5ï¼ˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹3ã¨4ï¼‰ã®ã¿ã‚’ä½¿ç”¨
                X_mfcc.append(mfcc_mean[3:5])  # 4ç•ªç›®ã¨5ç•ªç›®ã®ä¿‚æ•°ã®ã¿
                
                # ãƒ•ã‚©ãƒ«ãƒãƒ³ãƒˆæŠ½å‡º
                formants = extract_formants(y_data, sr)
                X_formants.append(formants)
                
                # å€‹åˆ¥ã‚µãƒ³ãƒ—ãƒ«ã®ãƒ•ã‚©ãƒ«ãƒãƒ³ãƒˆã‚’ä¿å­˜
                all_formants[vowel].append(formants)
                
                y.append(vowel)
                samples_found += 1
                
                print(f"âœ… {vowel}: ã‚µãƒ³ãƒ—ãƒ« {i} ã‚’èª­ã¿è¾¼ã¿")
            else:
                print(f"âš ï¸ æ—¥æœ¬èªæ¯éŸ³ {vowel} ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {filepath}")
        
        if samples_found > 0:
            print(f"ğŸ“Š {vowel}: {samples_found}å€‹ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’ç™»éŒ²")
    
    # è‹±èªæ¯éŸ³ã®å‡¦ç†ï¼ˆå®Ÿéš›ã®éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿ï¼‰
    print("\nğŸ‡¬ğŸ‡§ è‹±èªæ¯éŸ³ã‚µãƒ³ãƒ—ãƒ«ã‚’æ¤œç´¢ä¸­...")
    for vowel in ENGLISH_VOWELS:
        all_formants[vowel] = []
        
        # æ§˜ã€…ãªãƒ•ã‚¡ã‚¤ãƒ«åãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©¦ã™
        patterns = [
            f"*_women_{vowel}.wav",  # FUsAoaI8QFg_women_Ã¦.wav
            f"*_{vowel}.wav",  # ä»–ã®ãƒ‘ã‚¿ãƒ¼ãƒ³
        ]
        
        samples_found = 0
        for pattern in patterns:
            import glob
            files = glob.glob(os.path.join(RECORDINGS_DIR, pattern))
            for filepath in files[:SAMPLES_PER_VOWEL]:  # æœ€å¤§SAMPLES_PER_VOWELå€‹ã¾ã§
                if os.path.exists(filepath):
                    try:
                        # éŸ³å£°èª­ã¿è¾¼ã¿
                        y_data, sr = librosa.load(filepath, sr=RATE)
                        
                        # ç„¡éŸ³åŒºé–“é™¤å»
                        y_data, _ = librosa.effects.trim(y_data, top_db=20)
                        
                        # MFCCæŠ½å‡º
                        mfcc = librosa.feature.mfcc(y=y_data, sr=sr, n_mfcc=N_MFCC)
                        mfcc_mean = np.mean(mfcc, axis=1)
                        X_mfcc.append(mfcc_mean[3:5])
                        
                        # ãƒ•ã‚©ãƒ«ãƒãƒ³ãƒˆæŠ½å‡º
                        formants = extract_formants(y_data, sr)
                        X_formants.append(formants)
                        
                        # å€‹åˆ¥ã‚µãƒ³ãƒ—ãƒ«ã®ãƒ•ã‚©ãƒ«ãƒãƒ³ãƒˆã‚’ä¿å­˜
                        all_formants[vowel].append(formants)
                        
                        y.append(vowel)
                        samples_found += 1
                        
                        print(f"âœ… {vowel}: {os.path.basename(filepath)} ã‚’èª­ã¿è¾¼ã¿")
                        break  # æ¬¡ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¸
                    except Exception as e:
                        print(f"âš ï¸ {vowel}: {os.path.basename(filepath)} ã®èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
                
                if samples_found >= 1:  # 1ã¤ã§ã‚‚è¦‹ã¤ã‹ã£ãŸã‚‰æ¬¡ã®æ¯éŸ³ã¸
                    break
        
        if samples_found > 0:
            print(f"ğŸ“Š {vowel}: {samples_found}å€‹ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’ç™»éŒ²")
        else:
            print(f"âŒ {vowel}: ã‚µãƒ³ãƒ—ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    
    return np.array(X_mfcc), np.array(X_formants), np.array(y), all_formants

# === è‹±èªæ¯éŸ³ã®ç†è«–çš„ãªãƒ•ã‚©ãƒ«ãƒãƒ³ãƒˆå€¤ï¼ˆHzï¼‰ã¨è¡¨è¨˜æƒ…å ± ===
ENGLISH_FORMANT_DATA = {
    'Ã¦': [660, 1720],  # cat
    'Éª': [400, 2000],  # sit
    'ÊŠ': [440, 1020],  # put
    'É›': [530, 1840],  # get
    'É”': [570, 840],   # caught
    'ÊŒ': [640, 1190],  # but
    'É‘': [730, 1090]   # father
}

# === è‹±èªæ¯éŸ³ã®è¡¨ç¤ºãƒ©ãƒ™ãƒ«ï¼ˆãƒ•ã‚©ãƒ³ãƒˆå¯¾å¿œï¼‰ ===
ENGLISH_LABELS = {
    'Ã¦': 'ae',  # cat
    'Éª': 'I',   # sit
    'ÊŠ': 'U',   # put  
    'É›': 'E',   # get
    'É”': 'O',   # caught
    'ÊŒ': 'V',   # but
    'É‘': 'A'    # father
}

# === è‹±èªæ¯éŸ³ã®èª¬æ˜ ===
ENGLISH_DESCRIPTIONS = {
    'Ã¦': 'cat',
    'Éª': 'sit',
    'ÊŠ': 'put',
    'É›': 'get', 
    'É”': 'caught',
    'ÊŒ': 'but',
    'É‘': 'father'
}

# === ãƒ•ã‚©ãƒ«ãƒãƒ³ãƒˆå€¤ã‹ã‚‰MFCC4,5ã®è¿‘ä¼¼å€¤ã‚’è¨ˆç®—ã™ã‚‹é–¢æ•° ===
def formant_to_mfcc45(f1, f2, jp_templates=None):
    """ãƒ•ã‚©ãƒ«ãƒãƒ³ãƒˆå€¤ã‹ã‚‰MFCC4,5ã®è¿‘ä¼¼å€¤ã‚’è¨ˆç®—ï¼ˆæ—¥æœ¬èªæ¯éŸ³ã®å®Ÿæ¸¬å€¤ãƒ™ãƒ¼ã‚¹ï¼‰"""
    # æ—¥æœ¬èªæ¯éŸ³ã®å®Ÿæ¸¬å€¤ã‹ã‚‰å‹•çš„ã«å¤‰æ›å¼ã‚’ç”Ÿæˆ
    if jp_templates is not None:
        # æ—¥æœ¬èªæ¯éŸ³ã®F1,F2ã®ç¯„å›²ã‚’å–å¾—
        jp_f1_min, jp_f1_max = 300, 800  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
        jp_f2_min, jp_f2_max = 800, 2500
        jp_mfcc4_min, jp_mfcc4_max = -30, 70
        jp_mfcc5_min, jp_mfcc5_max = -25, 25
        
        # å®Ÿéš›ã®æ—¥æœ¬èªæ¯éŸ³ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç¯„å›²ã‚’æ¨å®š
        if 'a' in jp_templates and 'i' in jp_templates:
            # aã¯é€šå¸¸F1ãŒé«˜ãã€iã¯F2ãŒé«˜ã„
            jp_mfcc4_range = jp_templates['a'][0] - jp_templates['i'][0]
            jp_mfcc5_range = jp_templates['i'][1] - jp_templates['a'][1]
            
            # ã‚ˆã‚Šæ­£ç¢ºãªå¤‰æ›ä¿‚æ•°ã‚’è¨ˆç®—
            f1_to_mfcc4_scale = jp_mfcc4_range / (jp_f1_max - jp_f1_min) if jp_mfcc4_range != 0 else -0.1
            f2_to_mfcc5_scale = jp_mfcc5_range / (jp_f2_max - jp_f2_min) if jp_mfcc5_range != 0 else -0.02
            
            # ä¸­å¿ƒä½ç½®ã‚’è¨ˆç®—
            jp_center_mfcc4 = np.mean([jp_templates[v][0] for v in JAPANESE_VOWELS if v in jp_templates])
            jp_center_mfcc5 = np.mean([jp_templates[v][1] for v in JAPANESE_VOWELS if v in jp_templates])
            
            # å¤‰æ›å¼ã‚’é©ç”¨
            mfcc4 = jp_center_mfcc4 - (f1 - 550) * f1_to_mfcc4_scale
            mfcc5 = jp_center_mfcc5 - (f2 - 1650) * f2_to_mfcc5_scale
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šå¾“æ¥ã®å¤‰æ›å¼
            mfcc4 = 80 - (f1 / 10)
            mfcc5 = 35 - (f2 / 80)
    else:
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šå¾“æ¥ã®å¤‰æ›å¼
        mfcc4 = 80 - (f1 / 10)
        mfcc5 = 35 - (f2 / 80)
    
    return np.array([mfcc4, mfcc5])

# === æ¯éŸ³ã”ã¨ã«MFCCã¨ãƒ•ã‚©ãƒ«ãƒãƒ³ãƒˆã®å¹³å‡ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½œæˆ ===
def build_templates(X_mfcc, X_formants, y):
    templates = {}
    formant_templates = {}
    
    print("\nğŸ“ ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆä½œæˆä¸­...")
    
    # æ—¥æœ¬èªæ¯éŸ³ã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆä½œæˆï¼ˆéŸ³å£°ã‚µãƒ³ãƒ—ãƒ«ã‹ã‚‰ï¼‰
    samples_used = {}
    for vowel in JAPANESE_VOWELS:
        indices = y == vowel
        if np.any(indices):
            templates[vowel] = np.mean(X_mfcc[indices], axis=0)
            formant_templates[vowel] = np.mean(X_formants[indices], axis=0)
            samples_used[vowel] = np.sum(indices)
            print(f"âœ… æ—¥æœ¬èª {vowel}: {samples_used[vowel]}å€‹ã®ã‚µãƒ³ãƒ—ãƒ«ã‹ã‚‰ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆä½œæˆ")
    
    # è‹±èªæ¯éŸ³ã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆä½œæˆï¼ˆå®Ÿéš›ã®éŸ³å£°ã‚µãƒ³ãƒ—ãƒ«ã¾ãŸã¯ç†è«–å€¤ã‹ã‚‰ï¼‰
    for eng_vowel in ENGLISH_VOWELS:
        indices = y == eng_vowel
        if np.any(indices):  # å®Ÿéš›ã®ã‚µãƒ³ãƒ—ãƒ«ãŒã‚ã‚‹å ´åˆ
            templates[eng_vowel] = np.mean(X_mfcc[indices], axis=0)
            formant_templates[eng_vowel] = np.mean(X_formants[indices], axis=0)
            samples_used[eng_vowel] = np.sum(indices)
            print(f"âœ… è‹±èª {eng_vowel}: {samples_used[eng_vowel]}å€‹ã®ã‚µãƒ³ãƒ—ãƒ«ã‹ã‚‰ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆä½œæˆ")
        elif eng_vowel in ENGLISH_FORMANT_DATA:  # ã‚µãƒ³ãƒ—ãƒ«ãŒãªã„å ´åˆã¯ç†è«–å€¤ã‚’ä½¿ç”¨
            print(f"ğŸ”„ {eng_vowel}: ã‚µãƒ³ãƒ—ãƒ«ãŒãªã„ãŸã‚ç†è«–å€¤ã‹ã‚‰æ¨å®š...")
            formants = ENGLISH_FORMANT_DATA[eng_vowel]
            formant_templates[eng_vowel] = np.array(formants + [0])  # F3ã¯0ã§åˆæœŸåŒ–
            
            # æ—¥æœ¬èªæ¯éŸ³ã®å®Ÿæ¸¬å€¤ã‚’ä½¿ã£ã¦ãƒ•ã‚©ãƒ«ãƒãƒ³ãƒˆå€¤ã‹ã‚‰MFCC4,5ã‚’æ¨å®š
            mfcc_pos = formant_to_mfcc45(formants[0], formants[1], templates)
            templates[eng_vowel] = mfcc_pos
            
            print(f"ğŸ”¤ {eng_vowel}: F1={formants[0]}Hz, F2={formants[1]}Hz â†’ MFCC4,5=({mfcc_pos[0]:.1f}, {mfcc_pos[1]:.1f})")
    
    print(f"\nâœ… ç·ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆæ•°: {len(templates)}")
    jp_count = len([v for v in templates if v in JAPANESE_VOWELS])
    en_count = len([v for v in templates if v in ENGLISH_VOWELS])
    en_sample_count = len([v for v in samples_used if v in ENGLISH_VOWELS])
    en_theory_count = en_count - en_sample_count
    
    print(f"  â— æ—¥æœ¬èª: {jp_count}/{len(JAPANESE_VOWELS)} (å…¨ã¦ã‚µãƒ³ãƒ—ãƒ«ã‹ã‚‰)")
    print(f"  â–² è‹±èª: {en_count}/{len(ENGLISH_VOWELS)} (å®Ÿã‚µãƒ³ãƒ—ãƒ«: {en_sample_count}, ç†è«–å€¤: {en_theory_count})")
    
    # æ›–æ˜§æ¯éŸ³ï¼ˆã‚·ãƒ¥ãƒ¯ãƒ¼ï¼‰ã®ç†è«–å€¤ã‚’è¨ˆç®—
    if jp_count >= len(JAPANESE_VOWELS):
        # æ—¥æœ¬èª5æ¯éŸ³ã®ãƒ•ã‚©ãƒ«ãƒãƒ³ãƒˆå€¤ã‹ã‚‰ä¸­å¿ƒå€¤ã‚’è¨ˆç®—
        jp_formants = [formant_templates[v] for v in JAPANESE_VOWELS if v in formant_templates]
        if jp_formants:
            jp_formants_array = np.array(jp_formants)
            schwa_formants = np.mean(jp_formants_array, axis=0)
            
            # æ›–æ˜§æ¯éŸ³ã®MFCCã¯æ—¥æœ¬èªæ¯éŸ³ã®å¹³å‡
            jp_mfcc = [templates[v] for v in JAPANESE_VOWELS if v in templates]
            if jp_mfcc:
                jp_mfcc_array = np.array(jp_mfcc)
                schwa_mfcc = np.mean(jp_mfcc_array, axis=0)
                
                # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã«è¿½åŠ 
                templates['É™'] = schwa_mfcc
                formant_templates['É™'] = schwa_formants
                
                print(f"ğŸ”² ã‚·ãƒ¥ãƒ¯ãƒ¼éŸ³: F1={schwa_formants[0]:.0f}Hz, F2={schwa_formants[1]:.0f}Hz (æ—¥æœ¬èª5æ¯éŸ³ã®å¹³å‡)")
    
    return templates, formant_templates


# === éŒ²éŸ³ã•ã‚ŒãŸéŸ³å£°ã‹ã‚‰ç‰¹å¾´ã‚’æŠ½å‡º ===
def extract_user_features(filepath):
    # éŸ³å£°èª­ã¿è¾¼ã¿
    y_data, sr = librosa.load(filepath, sr=RATE)
    
    # ç„¡éŸ³åŒºé–“é™¤å»
    y_data, _ = librosa.effects.trim(y_data, top_db=20)
    
    # MFCCæŠ½å‡º
    mfcc = librosa.feature.mfcc(y=y_data, sr=sr, n_mfcc=N_MFCC)
    mfcc_mean = np.mean(mfcc, axis=1)
    # MFCC4ã¨MFCC5ï¼ˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹3ã¨4ï¼‰ã®ã¿ã‚’ä½¿ç”¨
    mfcc_features = mfcc_mean[3:5]  # 4ç•ªç›®ã¨5ç•ªç›®ã®ä¿‚æ•°ã®ã¿
    
    # ãƒ•ã‚©ãƒ«ãƒãƒ³ãƒˆæŠ½å‡º
    formant_features = extract_formants(y_data, sr)
    
    return mfcc_features, formant_features

# === ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®MFCCã¨ãƒ•ã‚©ãƒ«ãƒãƒ³ãƒˆã‚’ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã¨æ¯”è¼ƒã—ã¦åˆ†é¡ ===
def classify(user_mfcc, user_formants, templates, formant_templates, all_formants=None):
    # MFCCè·é›¢ã®è¨ˆç®—
    mfcc_distances = {vowel: np.linalg.norm(user_mfcc - vec) for vowel, vec in templates.items()}
    
    # ã‚µãƒ³ãƒ—ãƒ«ã®å€‹åˆ¥ãƒ•ã‚©ãƒ«ãƒãƒ³ãƒˆã¨ã®æ¯”è¼ƒï¼ˆé¡ä¼¼åº¦è¨ˆç®—ï¼‰
    sample_similarities = {}
    very_similar_vowel = None
    max_formant = 4000  # æ­£è¦åŒ–ã®ãŸã‚ã®æœ€å¤§ãƒ•ã‚©ãƒ«ãƒãƒ³ãƒˆå‘¨æ³¢æ•°
    
    if all_formants is not None:
        # æ—¥æœ¬èªã¨è‹±èªä¸¡æ–¹ã®æ¯éŸ³ã‚’ãƒã‚§ãƒƒã‚¯
        for vowel in VOWELS:
            vowel_samples = all_formants.get(vowel, [])
            if vowel_samples:
                # å„ã‚µãƒ³ãƒ—ãƒ«ã¨ã®é¡ä¼¼åº¦ã‚’è¨ˆç®—
                similarities = []
                for sample_formant in vowel_samples:
                    if len(sample_formant) >= 2 and len(user_formants) >= 2:
                        # F1ã¨F2ã®è·é›¢ã‚’è¨ˆç®—
                        f1_dist = abs(user_formants[0] - sample_formant[0]) / max_formant
                        f2_dist = abs(user_formants[1] - sample_formant[1]) / max_formant
                        
                        # è·é›¢ã‹ã‚‰é¡ä¼¼åº¦ã‚’è¨ˆç®—
                        dist = 0.5 * f1_dist + 0.5 * f2_dist
                        
                        sim = 1.0 / (1.0 + 10 * dist)
                        similarities.append(sim)
                
                # æœ€ã‚‚é¡ä¼¼åº¦ã®é«˜ã„ã‚µãƒ³ãƒ—ãƒ«ã‚’é¸æŠ
                if similarities:
                    max_sim = max(similarities)
                    
                    # é¡ä¼¼åº¦ã¯ãã®ã¾ã¾ä½¿ç”¨
                    
                    sample_similarities[vowel] = max_sim
                    
                    # éå¸¸ã«é«˜ã„é¡ä¼¼åº¦ã®å ´åˆã€ãã®æ¯éŸ³ã‚’è¨˜éŒ²
                    if max_sim > VERY_SIMILAR_THRESHOLD and (very_similar_vowel is None or max_sim > sample_similarities.get(very_similar_vowel, 0)):
                        very_similar_vowel = vowel
    
    # MFCCè·é›¢ã®ã¿ã‚’ä½¿ç”¨ï¼ˆãƒ•ã‚©ãƒ«ãƒãƒ³ãƒˆã¯é¡ä¼¼åº¦åˆ¤å®šã®ã¿ï¼‰
    combined_distances = mfcc_distances.copy()
    
    # è·é›¢ãŒè¿‘ã„é †ã«ã‚½ãƒ¼ãƒˆ
    sorted_distances = sorted(combined_distances.items(), key=lambda x: x[1])
    
    # ã‚µãƒ³ãƒ—ãƒ«ã¨ã®é¡ä¼¼åº¦ãŒéå¸¸ã«é«˜ã„å ´åˆã€ãã®æ¯éŸ³ã‚’å„ªå…ˆ
    if very_similar_vowel is not None:
        print(f"\nâ­ ã‚µãƒ³ãƒ—ãƒ«ã¨ã®é«˜ã„é¡ä¼¼åº¦æ¤œå‡º: ã€Œ{very_similar_vowel}ã€ (é¡ä¼¼åº¦: {sample_similarities[very_similar_vowel]:.3f})")
        # è©²å½“ã®æ¯éŸ³ã‚’å…ˆé ­ã«æŒã£ã¦ãã‚‹
        sorted_distances = [(very_similar_vowel, combined_distances[very_similar_vowel])] + \
                          [d for d in sorted_distances if d[0] != very_similar_vowel]
    
    # è·é›¢ãŒè¿‘ã„é †ã«ã‚½ãƒ¼ãƒˆæ¸ˆã¿
    
    # åˆ¤åˆ¥çµæœã¨ã¨ã‚‚ã«å„ç¨®æƒ…å ±ã‚’è¿”ã™
    return sorted_distances, mfcc_distances, sample_similarities if 'sample_similarities' in locals() else {}


# === 2æ¬¡å…ƒãƒ—ãƒ­ãƒƒãƒˆã®åˆæœŸåŒ–ï¼ˆMFCC4,5ã®ã¿ä½¿ç”¨ï¼‰ ===
def init_2d_plot(X, y, templates):
    # ç›´æ¥MFCC4,5ã‚’ãƒ—ãƒ­ãƒƒãƒˆ
    X_plot = X  # ã™ã§ã«MFCC4,5ã®2æ¬¡å…ƒ
    
    fig = plt.figure(figsize=(15, 10))
    ax = fig.add_subplot(111)
    
    # èƒŒæ™¯è‰²ã‚’è¨­å®š
    fig.patch.set_facecolor('white')
    ax.set_facecolor('#f8f9fa')
    
    # æ—¥æœ¬èªæ¯éŸ³ã‚¯ãƒ©ã‚¹ã‚¿ã‚’ãƒ—ãƒ­ãƒƒãƒˆï¼ˆå®Ÿéš›ã®ã‚µãƒ³ãƒ—ãƒ«ä½ç½®ï¼‰
    for vowel in JAPANESE_VOWELS:
        cluster = X_plot[y == vowel]
        if len(cluster) > 0:
            color = COLOR_MAP.get(vowel, 'gray')
            # å®Ÿéš›ã®ã‚µãƒ³ãƒ—ãƒ«ãƒã‚¤ãƒ³ãƒˆã‚’å°ã•ãè¡¨ç¤ºï¼ˆå‚è€ƒç”¨ï¼‰
            ax.scatter(cluster[:, 1], cluster[:, 0],  # Xè»¸ã¨Yè»¸ã‚’å…¥ã‚Œæ›¿ãˆ
                      s=60, color=color, alpha=0.5, 
                      edgecolor='gray', linewidth=1, zorder=3)
    
    # æ—¥æœ¬èªæ¯éŸ³ã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆä½ç½®ã‚’ãƒ—ãƒ­ãƒƒãƒˆï¼ˆç†æƒ³çš„ãªä½ç½®ï¼‰
    for vowel in JAPANESE_VOWELS:
        if vowel in templates:
            template_point = templates[vowel]
            color = COLOR_MAP.get(vowel, 'gray')
            
            # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆä½ç½®ã«å¤§ããªä¸¸ã‚’è¡¨ç¤º
            ax.scatter(template_point[1], template_point[0],  # Xè»¸ã¨Yè»¸ã‚’å…¥ã‚Œæ›¿ãˆ
                      label=f'{vowel} (æ—¥æœ¬èª)', s=300, color=color, alpha=0.9, 
                      edgecolor='white', linewidth=3, zorder=8)
            
            # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆä½ç½®ã®ä¸­å¿ƒã«æ¯éŸ³ãƒ©ãƒ™ãƒ«ã‚’è¡¨ç¤º
            ax.text(template_point[1], template_point[0], vowel,  # Xè»¸ã¨Yè»¸ã‚’å…¥ã‚Œæ›¿ãˆ
                    fontsize=18, weight='bold', color='white', ha='center', va='center',
                    bbox=dict(boxstyle='round,pad=0.4', facecolor=color, alpha=0.95, 
                             edgecolor='white', linewidth=2), zorder=9)
            
            # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆä½ç½®ã‚’ä¸­å¿ƒã¨ã—ãŸå††ã‚’è¡¨ç¤º
            circle = plt.Circle((template_point[1], template_point[0]), radius=5, 
                              fill=False, edgecolor=color, linewidth=2, 
                              alpha=0.6, linestyle=':', zorder=7)
            ax.add_patch(circle)
            
            # æ¯éŸ³åã‚’å¤–å´ã«è¡¨ç¤ºï¼ˆã‚ˆã‚Šç›®ç«‹ã¤ã‚ˆã†ã«ï¼‰
            ax.text(template_point[1] + 3, template_point[0] + 3, f'/{vowel}/', 
                    fontsize=12, weight='bold', color=color, ha='left', va='bottom',
                    bbox=dict(boxstyle='round,pad=0.2', facecolor='white', alpha=0.9, 
                             edgecolor=color, linewidth=1), zorder=10)
            
            print(f"  âœ… {vowel} ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆä½ç½®: ({template_point[0]:.1f}, {template_point[1]:.1f})")
    
    # è‹±èªæ¯éŸ³ã®ç†è«–ä½ç½®ã‚’å¼·åˆ¶çš„ã«ãƒ—ãƒ­ãƒƒãƒˆ
    eng_count = len([v for v in templates.keys() if v in ENGLISH_VOWELS])
    print(f"\nğŸ”§ è‹±èªæ¯éŸ³ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ãƒ—ãƒ­ãƒƒãƒˆä¸­... (ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆæ•°: {eng_count}/{len(ENGLISH_VOWELS)})")
    
    if eng_count == 0:
        print("    âš ï¸ è‹±èªæ¯éŸ³ã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãŒä½œæˆã•ã‚Œã¦ã„ã¾ã›ã‚“ï¼")
        print(f"    ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚­ãƒ¼: {list(templates.keys())}")
    
    for eng_vowel in ENGLISH_VOWELS:
        if eng_vowel in templates:
            point = templates[eng_vowel]
            color = COLOR_MAP.get(eng_vowel, 'gray')
            print(f"  ğŸ“ {eng_vowel}: ä½ç½®=({point[0]:.1f}, {point[1]:.1f}), è‰²={color}")
            
            # å›³ã®ç¯„å›²å†…ã‹ãƒãƒ§ãƒƒã‚¯
            if -30 <= point[0] <= 80 and -30 <= point[1] <= 30:
                # ã‚ˆã‚Šå¤§ããã€è¦‹ã‚„ã™ã„ä¸‰è§’å½¢ãƒãƒ¼ã‚«ãƒ¼ã§è‹±èªæ¯éŸ³ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ãƒ—ãƒ­ãƒƒãƒˆ
                ax.scatter(point[1], point[0],  # Xè»¸ã¨Yè»¸ã‚’å…¥ã‚Œæ›¿ãˆ
                          label=f'{ENGLISH_LABELS.get(eng_vowel, eng_vowel)} ({ENGLISH_DESCRIPTIONS.get(eng_vowel, "EN")})', 
                          s=350, color=color, alpha=0.9, 
                          marker='^', edgecolor='white', linewidth=3, zorder=10)
                
                # ã‚ˆã‚Šå¤§ããè¦‹ã‚„ã™ã„ãƒ†ã‚­ã‚¹ãƒˆãƒ©ãƒ™ãƒ«
                display_label = ENGLISH_LABELS.get(eng_vowel, eng_vowel)
                
                ax.text(point[1], point[0], display_label,  # Xè»¸ã¨Yè»¸ã‚’å…¥ã‚Œæ›¿ãˆ
                        fontsize=16, weight='bold', color='white', ha='center', va='center',
                        bbox=dict(boxstyle='round,pad=0.3', facecolor=color, alpha=0.95, 
                                 edgecolor='white', linewidth=2),
                        zorder=11)
                
                # è‹±èªæ¯éŸ³ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆä½ç½®ã‚’ä¸­å¿ƒã¨ã—ãŸå††ã‚’è¡¨ç¤º
                circle = plt.Circle((point[1], point[0]), radius=4, 
                                  fill=False, edgecolor=color, linewidth=2, 
                                  alpha=0.6, linestyle='--', zorder=9)
                ax.add_patch(circle)
                
                # è‹±èªæ¯éŸ³ã®èª¬æ˜ã‚’å¤–å´ã«è¡¨ç¤º
                desc = ENGLISH_DESCRIPTIONS.get(eng_vowel, eng_vowel)
                ax.text(point[1] + 3, point[0] - 3, f'/{desc}/', 
                        fontsize=10, weight='bold', color=color, ha='left', va='top',
                        bbox=dict(boxstyle='round,pad=0.15', facecolor='lightyellow', alpha=0.8, 
                                 edgecolor=color, linewidth=1), zorder=12)
                
                print(f"    âœ… {eng_vowel} ã‚’ãƒ—ãƒ­ãƒƒãƒˆå®Œäº†")
            else:
                print(f"    âš ï¸ {eng_vowel} ãŒç¯„å›²å¤–: ({point[0]:.1f}, {point[1]:.1f})")
        else:
            print(f"    âŒ {eng_vowel} ãŒãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    
    # æ›–æ˜§æ¯éŸ³ã®ç†è«–ä½ç½®ã‚’ãƒ—ãƒ­ãƒƒãƒˆ
    if 'É™' in templates:
        schwa_point = templates['É™']  # ã™ã§ã«2æ¬¡å…ƒ
        color = COLOR_MAP.get('É™', 'gray')
        ax.scatter(schwa_point[1], schwa_point[0],  # Xè»¸ã¨Yè»¸ã‚’å…¥ã‚Œæ›¿ãˆ
                  label='É™ (ã‚·ãƒ¥ãƒ¯ãƒ¼)', s=300, color=color, alpha=0.8, 
                  marker='s', edgecolor='white', linewidth=3, zorder=7)
        ax.text(schwa_point[1], schwa_point[0], 'É™',  # Xè»¸ã¨Yè»¸ã‚’å…¥ã‚Œæ›¿ãˆ
                fontsize=16, weight='bold', color='white', ha='center', va='center',
                bbox=dict(boxstyle='round,pad=0.3', facecolor=color, alpha=0.9,
                         edgecolor='white', linewidth=2), zorder=8)
        
        # ã‚·ãƒ¥ãƒ¯ãƒ¼éŸ³ã«ã‚‚å††ã¨èª¬æ˜ã‚’è¿½åŠ 
        circle = plt.Circle((schwa_point[1], schwa_point[0]), radius=4.5, 
                          fill=False, edgecolor=color, linewidth=2, 
                          alpha=0.4, linestyle='-.', zorder=6)
        ax.add_patch(circle)
        
        ax.text(schwa_point[1] + 3, schwa_point[0], '/schwa/', 
                fontsize=10, weight='bold', color=color, ha='left', va='center',
                bbox=dict(boxstyle='round,pad=0.15', facecolor='lightcyan', alpha=0.8, 
                         edgecolor=color, linewidth=1), zorder=9)
    
    # ã‚°ãƒ©ãƒ•ã®è¨­å®šï¼ˆè»¸ãƒ©ãƒ™ãƒ«ã‚‚å…¥ã‚Œæ›¿ãˆï¼‰
    ax.set_title("ğŸ¯ MFCC4-5 æ¯éŸ³èªè­˜ç©ºé–“\n" + 
                "ğŸ”µ æ—¥æœ¬èªæ¯éŸ³ã€€ğŸ”º è‹±èªæ¯éŸ³ã€€ğŸ”² ã‚·ãƒ¥ãƒ¯ãƒ¼éŸ³ã€€â­ ã‚ãªãŸã®ç™ºéŸ³", 
                fontsize=18, pad=25, weight='bold')
    ax.set_xlabel("MFCC5ï¼ˆç¬¬6ä¿‚æ•°ï¼‰", fontsize=16, weight='bold')
    ax.set_ylabel("MFCC4ï¼ˆç¬¬5ä¿‚æ•°ï¼‰", fontsize=16, weight='bold')
    
    # ã‚ˆã‚Šè¦‹ã‚„ã™ã„å‡¡ä¾‹è¨­å®š
    handles, labels = ax.get_legend_handles_labels()
    # æ—¥æœ¬èªã¨è‹±èªã§åˆ†ã‘ã¦è¡¨ç¤º
    jp_items = [(h, l) for h, l in zip(handles, labels) if 'æ—¥æœ¬èª' in l or 'ã‚·ãƒ¥ãƒ¯ãƒ¼' in l]
    en_items = [(h, l) for h, l in zip(handles, labels) if '(' in l and 'æ—¥æœ¬èª' not in l and 'ã‚·ãƒ¥ãƒ¯ãƒ¼' not in l]
    
    if jp_items:
        jp_legend = ax.legend([h for h, l in jp_items], [l for h, l in jp_items],
                             bbox_to_anchor=(1.02, 1), loc='upper left', 
                             title='ğŸ”µ æ—¥æœ¬èªæ¯éŸ³', title_fontsize=14, fontsize=12,
                             frameon=True, fancybox=True, shadow=True)
        jp_legend.get_title().set_weight('bold')
        plt.gca().add_artist(jp_legend)
    
    if en_items:
        en_legend = ax.legend([h for h, l in en_items], [l for h, l in en_items],
                             bbox_to_anchor=(1.02, 0.55), loc='upper left',
                             title='ğŸ”º è‹±èªæ¯éŸ³', title_fontsize=14, fontsize=12,
                             frameon=True, fancybox=True, shadow=True)
        en_legend.get_title().set_weight('bold')
    
    # ã‚ˆã‚Šè¦‹ã‚„ã™ã„ã‚°ãƒªãƒƒãƒ‰
    ax.grid(True, linestyle='-', linewidth=0.5, alpha=0.3, color='lightgray')
    ax.grid(True, linestyle=':', linewidth=1, alpha=0.5, color='blue', which='major')
    
    # è»¸ã®ç¯„å›²ã‚’æ‰‹å‹•ã§è¨­å®šã—ã¦å…¨ã¦ã®ç‚¹ãŒè¦‹ãˆã‚‹ã‚ˆã†ã«ã™ã‚‹
    all_points = list(templates.values())
    if all_points:
        all_points = np.array(all_points)
        x_margin = max(5, (all_points[:, 1].max() - all_points[:, 1].min()) * 0.15)
        y_margin = max(5, (all_points[:, 0].max() - all_points[:, 0].min()) * 0.15)
        ax.set_xlim(all_points[:, 1].min() - x_margin, all_points[:, 1].max() + x_margin)
        ax.set_ylim(all_points[:, 0].min() - y_margin, all_points[:, 0].max() + y_margin)

    print(f"\nğŸ“Š ãƒ—ãƒ­ãƒƒãƒˆå®Œäº† - ç·ãƒã‚¤ãƒ³ãƒˆæ•°: {len(templates)}")
    
    # è»¸ã®è‰²ã¨å¤ªã•ã‚’èª¿æ•´
    ax.spines['bottom'].set_linewidth(2)
    ax.spines['left'].set_linewidth(2)
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)
    ax.spines['bottom'].set_color('#333333')
    ax.spines['left'].set_color('#333333')
    
    # è»¸ã®ãƒ©ãƒ™ãƒ«ã®è‰²ã‚’èª¿æ•´
    ax.tick_params(axis='both', which='major', labelsize=12, colors='#333333')
    
    # å›³ã‚’é©åˆ‡ãªã‚µã‚¤ã‚ºã«èª¿æ•´
    plt.tight_layout()
    plt.subplots_adjust(right=0.7)  # å‡¡ä¾‹ç”¨ã®ã‚¹ãƒšãƒ¼ã‚¹ã‚’ç¢ºä¿
    
    return fig, ax, X_plot

# === ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ¯éŸ³ç‚¹ã‚’ãƒ—ãƒ­ãƒƒãƒˆã«è¿½åŠ ãƒ»æ›´æ–°ã™ã‚‹ ===
def update_user_point(ax, user_vec, predicted_label, score, prev_scatter=None, templates=None):
    # ä»¥å‰ã®ç‚¹ã‚’å‰Šé™¤ï¼ˆå¸¸ã«1ã¤ã ã‘è¡¨ç¤ºï¼‰
    if prev_scatter:
        prev_scatter.remove()
    user_point = user_vec
    scatter = ax.scatter(user_point[1], user_point[0], color='red', s=400, marker='*', 
                        edgecolor='yellow', linewidth=3, zorder=20, alpha=0.9)
    
    # æœ€ã‚‚è¿‘ã„ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã¨ã®è·é›¢ã‚’è¨ˆç®—ã—ã¦è¡¨ç¤º
    distance_info = ""
    if templates and predicted_label in templates:
        target_point = templates[predicted_label]
        distance = np.linalg.norm(user_point - target_point)
        if distance < 5:
            distance_info = f" | ğŸ‰ ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã¾ã§ {distance:.1f} - ç´ æ™´ã‚‰ã—ã„ï¼"
        elif distance < 15:
            distance_info = f" | ğŸ˜Š ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã¾ã§ {distance:.1f} - è‰¯ã„æ„Ÿã˜ï¼"
        elif distance < 30:
            distance_info = f" | ğŸ’ª ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã¾ã§ {distance:.1f} - ã‚ã¨å°‘ã—ï¼"
        else:
            distance_info = f" | ğŸ¯ ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã¾ã§ {distance:.1f} - é ­å¼µã‚ã†ï¼"
    
    advice = ADVICE_MAP.get(predicted_label, "ç·´ç¿’ã‚’ç¶šã‘ã¾ã—ã‚‡ã†ã€‚")
    ax.set_title(f"æ¨å®š: ã€Œ{predicted_label}ã€ (è·é›¢: {score:.2f}){distance_info}\nğŸ’¡ {advice}", 
                fontsize=14, pad=10)
    plt.pause(0.01)
    return scatter

# === ç™ºéŸ³ã‚¹ã‚³ã‚¢ã«å¿œã˜ãŸã‚¢ãƒ‰ãƒã‚¤ã‚¹è¡¨ç¤º ===
def show_advice(vowel, score):
    print("\nğŸ§ª ç™ºéŸ³è©•ä¾¡:")
    if score < 15:
        print("âœ… ç™ºéŸ³ã¯è‰¯å¥½ã§ã™ï¼")
    elif score < 30:
        print("âš  å°‘ã—ã‚ºãƒ¬ã¦ã„ã¾ã™ã€‚ã‚‚ã†ä¸€åº¦æ„è­˜ã—ã¦ç™ºéŸ³ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚")
    else:
        print("âŒ ç™ºéŸ³ãŒã‹ãªã‚Šã‚ºãƒ¬ã¦ã„ã¾ã™ã€‚ä»¥ä¸‹ã®æ”¹å–„ç‚¹ã‚’å‚è€ƒã«ã—ã¦ãã ã•ã„ã€‚")
        print(f"ğŸ—£ ã€Œ{vowel}ã€ã®ç™ºéŸ³ã‚¢ãƒ‰ãƒã‚¤ã‚¹: {ADVICE_MAP.get(vowel, 'ç·´ç¿’ã‚’ç¶šã‘ã¾ã—ã‚‡ã†ã€‚')}")

# === MFCCç‰¹å¾´é‡ã®èª¬æ˜ ===
def show_mfcc_info():
    print("\nğŸ“Š ä½¿ç”¨ä¸­ã®ç‰¹å¾´é‡:")
    print("  MFCC4ï¼ˆç¬¬4ä¿‚æ•°ï¼‰: ä¸­åŸŸã‚¹ãƒšã‚¯ãƒˆãƒ«æ§‹é€ ")
    print("  MFCC5ï¼ˆç¬¬5ä¿‚æ•°ï¼‰: ä¸­åŸŸã‚¹ãƒšã‚¯ãƒˆãƒ«æ§‹é€ ")
    print("\nğŸ“– MFCC4,5ã®ç‰¹æ€§:")
    print("  ãƒ»ãƒ•ã‚©ãƒ«ãƒãƒ³ãƒˆå‘¨æ³¢æ•°ï¼ˆç‰¹ã«F1,F2ï¼‰ã¨é«˜ã„ç›¸é–¢")
    print("  ãƒ»æ¯éŸ³ã®éŸ³éŸ¿çš„ç‰¹å¾´ã‚’åŠ¹ç‡çš„ã«è¡¨ç¾")
    print("  ãƒ»ã‚¹ãƒšã‚¯ãƒˆãƒ«ã®ä¸­åŸŸï¼ˆç´„1000-3000Hzï¼‰ã®æƒ…å ±ã‚’ä¸»ã«åæ˜ ")


# === ãƒ•ã‚©ãƒ«ãƒãƒ³ãƒˆæƒ…å ±ã®è¡¨ç¤º ===
def display_formant_info(formant_templates):
    """å„æ¯éŸ³ã®ãƒ•ã‚©ãƒ«ãƒãƒ³ãƒˆæƒ…å ±ã‚’è¡¨ç¤º"""
    print("\nğŸ” æ¯éŸ³ã®ãƒ•ã‚©ãƒ«ãƒãƒ³ãƒˆæƒ…å ±:")
    for vowel, formants in formant_templates.items():
        if len(formants) >= 2:
            print(f"  ã€Œ{vowel}ã€: F1={formants[0]:.0f}Hz, F2={formants[1]:.0f}Hz")
            
            # ç‰¹ã«iã¨eã®é•ã„ã‚’è©³ç´°è¡¨ç¤º
            if vowel in ['i', 'e']:
                print(f"    ğŸ‘‰ ã€Œ{vowel}ã€ã®ç‰¹å¾´: {'é«˜ã„ç¬¬2ãƒ•ã‚©ãƒ«ãƒãƒ³ãƒˆ' if vowel == 'i' else 'ä¸­ç¨‹åº¦ã®ç¬¬2ãƒ•ã‚©ãƒ«ãƒãƒ³ãƒˆ'}")





# === ãƒ¡ã‚¤ãƒ³å‡¦ç† ===
def main():
    print("ğŸ“¦ ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆèª­ã¿è¾¼ã¿ä¸­...")
    X_mfcc, X_formants, y, all_formants = extract_features()
    
    if len(X_mfcc) == 0:
        print("âŒ ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãŒã‚ã‚Šã¾ã›ã‚“ã€‚recordings ãƒ•ã‚©ãƒ«ãƒ€ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        return

    # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆä½œæˆ
    templates, formant_templates = build_templates(X_mfcc, X_formants, y)
    
    # ãƒ•ã‚©ãƒ«ãƒãƒ³ãƒˆæƒ…å ±ã®è¡¨ç¤º
    display_formant_info(formant_templates)
    
    
    
    # MFCCæƒ…å ±è¡¨ç¤º
    show_mfcc_info()

    plt.ion()  # å¯¾è©±ãƒ¢ãƒ¼ãƒ‰ON
    _, ax, _ = init_2d_plot(X_mfcc, y, templates)

    print(f"\nğŸ’« å€‹åˆ¥ã‚µãƒ³ãƒ—ãƒ«ã¨ã®é¡ä¼¼åº¦æ¯”è¼ƒ: æœ‰åŠ¹ (é–¾å€¤: {VERY_SIMILAR_THRESHOLD})")
    print("\nğŸ“Š ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆä½ç½®æƒ…å ±:")
    print("  ğŸ”µ æ—¥æœ¬èªæ¯éŸ³ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ (å¤§ããªä¸¸ãƒ»ç†æƒ³çš„ãªä½ç½®):")
    for vowel, mfcc_pos in templates.items():
        if vowel in JAPANESE_VOWELS:
            print(f"    â— {vowel}: ({mfcc_pos[0]:.1f}, {mfcc_pos[1]:.1f}) - ã“ã®ä½ç½®ã‚’ç›®æ¨™ã«ç™ºéŸ³ç·´ç¿’")
    
    print("  ğŸ”º è‹±èªæ¯éŸ³ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ (ä¸‰è§’å½¢ãƒ»ç†æƒ³çš„ãªä½ç½®):")
    for vowel, mfcc_pos in templates.items():
        if vowel in ENGLISH_VOWELS:
            desc = ENGLISH_DESCRIPTIONS.get(vowel, vowel)
            print(f"    â–² {vowel} ({desc}): ({mfcc_pos[0]:.1f}, {mfcc_pos[1]:.1f}) - ã“ã®ä½ç½®ã‚’ç›®æ¨™ã«ç™ºéŸ³ç·´ç¿’")
    
    for vowel, mfcc_pos in templates.items():
        if vowel == 'É™':
            print(f"  ğŸ”² {vowel} (ã‚·ãƒ¥ãƒ¯ãƒ¼): ({mfcc_pos[0]:.1f}, {mfcc_pos[1]:.1f}) - ä¸­ç«‹çš„ãªéŸ³ã®ä½ç½®")
    
    print("\nğŸ“Œ MFCC4,5ãƒ™ãƒ¼ã‚¹ã®æ¯éŸ³ç™ºéŸ³ç·´ç¿’ã‚·ã‚¹ãƒ†ãƒ ")
    print("  â†’ MFCC4,5ã¯ä¸­åŸŸã®ã‚¹ãƒšã‚¯ãƒˆãƒ«æ§‹é€ ã‚’è¡¨ã—ã€ãƒ•ã‚©ãƒ«ãƒãƒ³ãƒˆæƒ…å ±ã¨ç›¸é–¢")
    print("  â†’ å¤§ããªãƒãƒ¼ã‚«ãƒ¼ = ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆä½ç½®ï¼ˆç†æƒ³çš„ãªç›®æ¨™ï¼‰")
    print("  â†’ å°ã•ãªãƒãƒ¼ã‚«ãƒ¼ = å®Ÿéš›ã®éŸ³å£°ã‚µãƒ³ãƒ—ãƒ«ï¼ˆå‚è€ƒç”¨ï¼‰")
    print("  â†’ ç ´ç·šã®å†† = ç™ºéŸ³ç·´ç¿’ã®ç›®æ¨™ç¯„å›²")
    print("ğŸ¯ ç›®æ¨™: å¤§ããªãƒãƒ¼ã‚«ãƒ¼ã®ä½ç½®ã«è¿‘ã¥ã‘ã‚‹ã‚ˆã†ã«ç™ºéŸ³ã—ã¦ãã ã•ã„ï¼")
    print("ğŸŸ¢ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¯éŸ³èªè­˜ã‚’é–‹å§‹ã—ã¾ã™ï¼ˆCtrl+Cã§åœæ­¢ï¼‰")
    prev_scatter = None
    last_predicted = None
    last_dist = None
    
    RATE = 16000
    FRAME_SIZE = int(RATE * 0.05)  # 0.05ç§’ãƒ•ãƒ¬ãƒ¼ãƒ 
    MIN_VOICE_FRAMES = int(0.15 / 0.05)  # 0.15ç§’ä»¥ä¸Šã®éŸ³å£°ã®ã¿åˆ¤å®š
    MAX_SILENCE_FRAMES = int(0.2 / 0.05)  # 0.2ç§’ä»¥ä¸Šç„¡éŸ³ã§éŸ³å£°åŒºé–“çµ‚äº†
    q = queue.Queue()

    def audio_callback(indata, *args):
        q.put(indata.copy())

    is_voice = False
    silence_count = 0
    voice_frames = []

    with sd.InputStream(samplerate=RATE, channels=1, callback=audio_callback, blocksize=FRAME_SIZE):
        try:
            while True:
                # ãƒãƒƒãƒ•ã‚¡ã«ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
                while not q.empty():
                    frame = q.get().flatten()
                    # éŸ³å£°åŒºé–“åˆ¤å®š
                    max_amplitude = np.max(np.abs(frame))
                    rms_energy = np.sqrt(np.mean(frame**2))
                    zcr = np.mean(librosa.feature.zero_crossing_rate(frame, frame_length=len(frame), hop_length=len(frame)//2))
                    # ã•ã‚‰ã«ã—ãã„å€¤ã‚’å’Œã‚‰ã’ã‚‹
                    is_valid = (max_amplitude > 0.03 and rms_energy > 0.005 and zcr < 0.30)
                    if is_valid:
                        voice_frames.append(frame)
                        is_voice = True
                        silence_count = 0
                    else:
                        if is_voice:
                            silence_count += 1
                            if silence_count >= MAX_SILENCE_FRAMES:
                                # éŸ³å£°åŒºé–“çµ‚äº†
                                if len(voice_frames) >= MIN_VOICE_FRAMES:
                                    voice_audio = np.concatenate(voice_frames)
                                    tmp_path = "_tmp_user_input.wav"
                                    sf.write(tmp_path, voice_audio, RATE)
                                    user_mfcc, user_formants = extract_user_features(tmp_path)
                                    os.remove(tmp_path)
                                    if user_mfcc is not None and user_formants is not None:
                                        results, _, sample_similarities = classify(
                                            user_mfcc, user_formants, templates, formant_templates, all_formants)
                                        predicted, dist = results[0]
                                        print("\n=== åˆ¤å®šçµæœ ===")
                                        print(f"ğŸ—£ æ¨å®š: ã€Œ{predicted}ã€ / è·é›¢ã‚¹ã‚³ã‚¢: {dist:.2f}")
                                        f1, f2 = user_formants[0], user_formants[1]
                                        print(f"ğŸ“Š ã‚ãªãŸã®ãƒ•ã‚©ãƒ«ãƒãƒ³ãƒˆ: F1={f1:.0f}Hz, F2={f2:.0f}Hz, F3={user_formants[2]:.0f}Hz")
                                        if sample_similarities:
                                            print("ğŸ“Š ã‚µãƒ³ãƒ—ãƒ«ã¨ã®é¡ä¼¼åº¦:")
                                            for vowel, sim in sorted(sample_similarities.items(), key=lambda x: x[1], reverse=True)[:3]:
                                                print(f"  ã€Œ{vowel}ã€: {sim:.3f}")
                                        print("ğŸ“Š é¡ä¼¼åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°:")
                                        for i, (v, d) in enumerate(results):
                                            print(f"  {i+1}. {v}ï¼ˆè·é›¢: {d:.2f}ï¼‰")
                                        show_advice(predicted, dist)
                                        prev_scatter = update_user_point(ax, user_mfcc, predicted, dist, prev_scatter, templates)
                                        last_predicted = predicted
                                        last_dist = dist
                                        plt.pause(0.01)
                                # ãƒãƒƒãƒ•ã‚¡ãƒªã‚»ãƒƒãƒˆ
                                voice_frames = []
                                is_voice = False
                                silence_count = 0
                        else:
                            # ç„¡éŸ³æ™‚ã‚‚ã‚°ãƒ©ãƒ•ãƒ»ç‚¹ãƒ»ã‚¿ã‚¤ãƒˆãƒ«ã¯ãã®ã¾ã¾
                            if last_predicted is not None and last_dist is not None:
                                advice = ADVICE_MAP.get(last_predicted, "ç·´ç¿’ã‚’ç¶šã‘ã¾ã—ã‚‡ã†ã€‚")
                                ax.set_title(f"æ¨å®šã•ã‚ŒãŸæ¯éŸ³: ã€Œ{last_predicted}ã€ (è·é›¢: {last_dist:.2f})\nğŸ’¡ {advice}\nğŸ”‡ ç„¡éŸ³ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ", fontsize=14, color='red')
                            else:
                                ax.set_title("ğŸ”‡ ç„¡éŸ³ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ", fontsize=16, color='red')
                            for text in ax.texts:
                                text.remove()
                            ax.text(0, 0, "ğŸ”‡\nç„¡éŸ³", fontsize=20, color='red', ha='center', va='center', alpha=0.8, weight='bold')
                            plt.pause(0.01)
                sleep(0.01)
        except KeyboardInterrupt:
            print("\nğŸ›‘ çµ‚äº†ã—ã¾ã—ãŸã€‚")
            plt.ioff()
            plt.close('all')

# === éŒ²éŸ³æ©Ÿèƒ½ ===
def record_vowels():
    """æ—¥æœ¬èªæ¯éŸ³ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’éŒ²éŸ³ã™ã‚‹"""
    vowels = ['a', 'i', 'u', 'e', 'o']
    os.makedirs(RECORDINGS_DIR, exist_ok=True)
    
    print("ğŸ™ï¸ æ—¥æœ¬èªæ¯éŸ³ã‚µãƒ³ãƒ—ãƒ«ã®éŒ²éŸ³ã‚’é–‹å§‹ã—ã¾ã™")
    print("å„æ¯éŸ³ã‚’3å›ãšã¤éŒ²éŸ³ã—ã¦ãã ã•ã„")
    print("éŒ²éŸ³æ™‚é–“: 1ç§’é–“")
    
    for vowel in vowels:
        print(f"\n=== ã€Œ{vowel}ã€ã®éŒ²éŸ³ ===")
        for i in range(SAMPLES_PER_VOWEL):
            print(f"â†’ã€Œ{vowel}ã€ã‚’ç™ºéŸ³ã—ã¦ãã ã•ã„ï¼ˆ{i+1}/{SAMPLES_PER_VOWEL}ï¼‰")
            print("3ç§’å¾Œã«éŒ²éŸ³é–‹å§‹...")
            
            # ã‚«ã‚¦ãƒ³ãƒˆãƒ€ã‚¦ãƒ³
            for j in range(3, 0, -1):
                print(f"{j}...")
                sleep(1)
            
            print("ğŸ”´ éŒ²éŸ³ä¸­...")
            audio = sd.rec(int(RATE * DURATION), samplerate=RATE, channels=1)
            sd.wait()
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
            filename = f"{RECORDINGS_DIR}/{vowel}_{i+1}.wav"
            sf.write(filename, audio, RATE)
            print(f"âœ… éŒ²éŸ³å®Œäº†: {filename}")
    
    print("\nğŸ‰ ã™ã¹ã¦ã®éŒ²éŸ³ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
    print("ã‚·ã‚¹ãƒ†ãƒ ã‚’å†èµ·å‹•ã—ã¦ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚")

def check_missing_samples():
    """ä¸è¶³ã—ã¦ã„ã‚‹æ—¥æœ¬èªæ¯éŸ³ã‚µãƒ³ãƒ—ãƒ«ã‚’ãƒã‚§ãƒƒã‚¯"""
    missing_vowels = []
    
    for vowel in JAPANESE_VOWELS:
        found_samples = 0
        for i in range(1, SAMPLES_PER_VOWEL + 1):
            filepath = os.path.join(RECORDINGS_DIR, f"{vowel}_{i}.wav")
            if os.path.exists(filepath):
                found_samples += 1
        
        if found_samples == 0:
            missing_vowels.append(vowel)
        elif found_samples < SAMPLES_PER_VOWEL:
            print(f"âš ï¸ {vowel}: {found_samples}/{SAMPLES_PER_VOWEL}å€‹ã®ã‚µãƒ³ãƒ—ãƒ«ã®ã¿å­˜åœ¨")
    
    return missing_vowels

# === ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ ===
if __name__ == "__main__":
    # recordingsãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å­˜åœ¨ç¢ºèª
    if not os.path.exists(RECORDINGS_DIR):
        print(f"ğŸ“ {RECORDINGS_DIR}ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆã—ã¾ã™...")
        os.makedirs(RECORDINGS_DIR)
    
    # ä¸è¶³ã‚µãƒ³ãƒ—ãƒ«ã®ãƒã‚§ãƒƒã‚¯
    missing_vowels = check_missing_samples()
    
    if missing_vowels:
        print(f"âŒ ä»¥ä¸‹ã®æ—¥æœ¬èªæ¯éŸ³ã®ã‚µãƒ³ãƒ—ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“: {missing_vowels}")
        
        # éŒ²éŸ³ã™ã‚‹ã‹ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ç¢ºèª
        response = input("\néŒ²éŸ³ã—ã¾ã™ã‹ï¼Ÿ (y/n): ").lower().strip()
        if response in ['y', 'yes', 'ã¯ã„']:
            record_vowels()
        else:
            print("âš ï¸ ã‚µãƒ³ãƒ—ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”¨æ„ã—ã¦ã‹ã‚‰å†å®Ÿè¡Œã—ã¦ãã ã•ã„")
    else:
        print("âœ… æ—¥æœ¬èªæ¯éŸ³ã‚µãƒ³ãƒ—ãƒ«ãŒæƒã£ã¦ã„ã¾ã™")
        main()